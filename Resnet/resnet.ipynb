{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7209801,"sourceType":"datasetVersion","datasetId":4171601}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    roc_curve, \n    auc, \n    precision_recall_curve, \n    average_precision_score\n)\nimport shutil\n\n# Memory and GPU configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass WeatherClassificationPipeline:\n    def __init__(self, dataset_path, output_path, target_size=(224, 224), sample_size=30000):\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.target_size = target_size\n        self.sample_size = sample_size\n        \n        # Predefined classes to ensure consistency\n        self.classes = ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\n\n    def extract_sample_dataset(self):\n        \"\"\"Extract a stratified sample of images for train and validation\"\"\"\n        print(\"\\n--- STEP: Extracting Sample Dataset ---\")\n        \n        # Create output directories\n        sample_train_path = os.path.join(self.output_path, 'train')\n        sample_val_path = os.path.join(self.output_path, 'val')\n        os.makedirs(sample_train_path, exist_ok=True)\n        os.makedirs(sample_val_path, exist_ok=True)\n        \n        # Calculate total samples and samples per class for train and val\n        train_sample_ratio = 0.8\n        val_sample_ratio = 0.2\n        total_samples_per_class = self.sample_size // len(self.classes)\n        train_samples_per_class = int(total_samples_per_class * train_sample_ratio)\n        val_samples_per_class = int(total_samples_per_class * val_sample_ratio)\n\n        for cls in self.classes:\n            # Create class directories\n            train_cls_path = os.path.join(sample_train_path, cls)\n            val_cls_path = os.path.join(sample_val_path, cls)\n            os.makedirs(train_cls_path, exist_ok=True)\n            os.makedirs(val_cls_path, exist_ok=True)\n\n            # Find source directory\n            cls_source_path = os.path.join(self.dataset_path, 'train', cls)\n            \n            # Get all image files\n            image_files = [f for f in os.listdir(cls_source_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Randomly shuffle images\n            np.random.shuffle(image_files)\n            \n            # Select train and validation samples\n            train_images = image_files[:train_samples_per_class]\n            val_images = image_files[train_samples_per_class:train_samples_per_class+val_samples_per_class]\n            \n            # Copy train images\n            for img in train_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(train_cls_path, img)\n                shutil.copy(src, dst)\n            \n            # Copy validation images\n            for img in val_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(val_cls_path, img)\n                shutil.copy(src, dst)\n            \n            print(f\"Class {cls}:\")\n            print(f\"  Train images: {len(train_images)}\")\n            print(f\"  Validation images: {len(val_images)}\")\n\n        return self.output_path\n\n    def prepare_data_generators(self, dataset_path):\n        \"\"\"Prepare data generators for training and validation\"\"\"\n        print(\"\\n--- STEP: Preparing Data Generator ---\")\n        \n        # Create ImageDataGenerator with preprocessing and augmentation\n        train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True\n        )\n\n        val_datagen = ImageDataGenerator(\n            rescale=1./255\n        )\n\n        # Train generator\n        train_generator = train_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'train'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=True,\n            classes=self.classes\n        )\n\n        # Validation generator\n        val_generator = val_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'val'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=False,\n            classes=self.classes\n        )\n        \n        print(f\"Classes: {self.classes}\")\n        print(f\"Number of training samples: {train_generator.samples}\")\n        print(f\"Number of validation samples: {val_generator.samples}\")\n\n        return train_generator, val_generator\n\n    def create_resnet_model(self):\n        \"\"\"Create custom ResNet-like model with fixed downsampling\"\"\"\n        def resnet_block(x, filters, kernel_size=3, downsample=False):\n            # Store original input for skip connection\n            shortcut = x\n            \n            # First convolution layer\n            x = layers.Conv2D(filters, kernel_size, padding='same', \n                              strides=2 if downsample else 1)(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.Activation('relu')(x)\n            \n            # Second convolution layer\n            x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n            x = layers.BatchNormalization()(x)\n            \n            # Adjust shortcut if downsampling\n            if downsample:\n                shortcut = layers.Conv2D(filters, 1, strides=2, padding='same')(shortcut)\n                shortcut = layers.BatchNormalization()(shortcut)\n            \n            # Merge shortcut and main path\n            x = layers.Add()([x, shortcut])\n            x = layers.Activation('relu')(x)\n            return x\n\n        # Input layer\n        inputs = layers.Input(shape=(*self.target_size, 3))\n        \n        # Initial convolution and pooling\n        x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation('relu')(x)\n        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n        # ResNet Blocks with proper downsampling\n        x = resnet_block(x, 64)\n        x = resnet_block(x, 128, downsample=True)\n        x = resnet_block(x, 256, downsample=True)\n\n        # Global average pooling and dense layers\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(512, activation='relu')(x)\n        x = layers.Dropout(0.5)(x)\n        outputs = layers.Dense(len(self.classes), activation='softmax')(x)\n\n        model = models.Model(inputs=inputs, outputs=outputs)\n        return model\n\n    def plot_training_curves(self, history):\n        \"\"\"Plot training and validation accuracy/loss\"\"\"\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(history.history['accuracy'], label='Training Accuracy')\n        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Model Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(history.history['loss'], label='Training Loss')\n        plt.plot(history.history['val_loss'], label='Validation Loss')\n        plt.title('Model Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig('training_curves.png')\n        plt.close()\n\n    def plot_confusion_matrix(self, val_generator, model):\n        \"\"\"Plot confusion matrix\"\"\"\n        # Get predictions\n        predictions = model.predict(val_generator)\n        y_pred = np.argmax(predictions, axis=1)\n        y_true = val_generator.classes\n\n        # Plot confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', \n                    xticklabels=self.classes, \n                    yticklabels=self.classes)\n        plt.title('Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.tight_layout()\n        plt.savefig('confusion_matrix.png')\n        plt.close()\n\n    def plot_roc_curve(self, val_generator, model):\n        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute ROC curve and ROC area for each class\n        n_classes = len(self.classes)\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        \n        # Binarize the output\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n            plt.plot(fpr[i], tpr[i], \n                     label=f'ROC curve (class: {self.classes[i]}, area = {roc_auc[i]:.2f})')\n        \n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend(loc=\"lower right\")\n        plt.tight_layout()\n        plt.savefig('roc_curve.png')\n        plt.close()\n\n    def plot_precision_recall_curve(self, val_generator, model):\n        \"\"\"Plot Precision-Recall curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute Precision-Recall curve\n        n_classes = len(self.classes)\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            avg_precision = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n            \n            plt.plot(recall, precision, \n                     label=f'Precision-Recall curve (class: {self.classes[i]}, AP = {avg_precision:.2f})')\n        \n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curve')\n        plt.legend(loc=\"best\")\n        plt.tight_layout()\n        plt.savefig('precision_recall_curve.png')\n        plt.close()\n\n    def generate_classification_report(self, val_generator, model):\n        \"\"\"Generate and save detailed classification report\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        y_true = val_generator.classes\n        \n        # Generate classification report\n        report = classification_report(\n            y_true, \n            y_pred, \n            target_names=self.classes, \n            output_dict=True\n        )\n        \n        # Create a DataFrame for better visualization\n        report_df = pd.DataFrame(report).transpose()\n        \n        # Plot as heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(report_df.iloc[:-2, :-1].astype(float), \n                    annot=True, \n                    cmap='YlGnBu', \n                    fmt='.2f')\n        plt.title('Classification Report Metrics')\n        plt.tight_layout()\n        plt.savefig('classification_report_heatmap.png')\n        plt.close()\n        \n        # Save textual report\n        with open('classification_report.txt', 'w') as f:\n            f.write(classification_report(\n                y_true, \n                y_pred, \n                target_names=self.classes\n            ))\n        \n        return report  \n\n    def visualize_sample_images(self, dataset_path):\n        \"\"\"Visualize sample images from each class\"\"\"\n        print(\"\\n--- STEP: Visualizing Sample Images ---\")\n        \n        # Create figure for sample images\n        plt.figure(figsize=(15, 10))\n        \n        # Get train directory\n        train_dir = os.path.join(dataset_path, 'train')\n        \n        # Iterate through classes\n        for i, cls in enumerate(self.classes, 1):\n            # Get path to class directory\n            cls_path = os.path.join(train_dir, cls)\n            \n            # Get list of image files\n            image_files = [f for f in os.listdir(cls_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Select first image\n            if image_files:\n                img_path = os.path.join(cls_path, image_files[0])\n                \n                # Load and display image\n                plt.subplot(2, 3, i)\n                img = plt.imread(img_path)\n                plt.imshow(img)\n                plt.title(cls)\n                plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('sample_images.png')\n        plt.close()\n\n    def predict_single_image(self, model, image_path):\n        \"\"\"Predict weather class for a single image\"\"\"\n        print(\"\\n--- STEP: Predicting Single Image ---\")\n        \n        # Load and preprocess the image\n        img = load_img(image_path, target_size=self.target_size)\n        img_array = img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize\n        \n        # Make prediction\n        predictions = model.predict(img_array)\n        predicted_class_index = np.argmax(predictions[0])\n        predicted_class = self.classes[predicted_class_index]\n        confidence = predictions[0][predicted_class_index]\n        \n        # Visualize prediction\n        plt.figure(figsize=(10, 5))\n        \n        # Original image\n        plt.subplot(1, 2, 1)\n        plt.imshow(plt.imread(image_path))\n        plt.title('Original Image')\n        plt.axis('off')\n        \n        # Prediction bar plot\n        plt.subplot(1, 2, 2)\n        plt.bar(self.classes, predictions[0])\n        plt.title('Class Probabilities')\n        plt.xlabel('Weather Classes')\n        plt.ylabel('Probability')\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig('single_image_prediction.png')\n        plt.close()\n        \n        # Print prediction details\n        print(f\"Predicted Class: {predicted_class}\")\n        print(f\"Confidence: {confidence:.2%}\")\n        \n        # Print full probabilities\n        for cls, prob in zip(self.classes, predictions[0]):\n            print(f\"{cls}: {prob:.2%}\")\n        \n        return predicted_class, confidence\n           \n    def run_pipeline(self):\n        # Extract sample dataset\n        sample_train_path = self.extract_sample_dataset()\n\n        # Visualize sample images\n        self.visualize_sample_images(sample_train_path)\n\n        # Prepare data generators\n        train_generator, val_generator = self.prepare_data_generators(sample_train_path)\n\n        # Create model\n        model = self.create_resnet_model()\n        \n        # Compile model\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=0.001),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        # Callbacks\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=5, \n            min_lr=0.00001\n        )\n        \n        early_stopping = EarlyStopping(\n            monitor='val_loss', \n            patience=10, \n            restore_best_weights=True\n        )\n\n        # Training\n        history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=50,\n            callbacks=[reduce_lr, early_stopping]\n        )\n\n        # Visualizations\n        self.plot_training_curves(history)\n        self.plot_confusion_matrix(val_generator, model)\n        self.plot_roc_curve(val_generator, model)\n        self.plot_precision_recall_curve(val_generator, model)\n        report = self.generate_classification_report(val_generator, model)\n\n        # Predict single image\n        test_image_path = '/kaggle/input/bdd100k-weather-classification/test/cabc30fc-e7726578.jpg'\n        self.predict_single_image(model, test_image_path)\n        \n        # Print out some key metrics from the report\n        print(\"\\nClassification Report Summary:\")\n        for cls in self.classes:\n            print(f\"{cls}:\")\n            print(f\"  Precision: {report[cls]['precision']:.4f}\")\n            print(f\"  Recall: {report[cls]['recall']:.4f}\")\n            print(f\"  F1-Score: {report[cls]['f1-score']:.4f}\")\n\n        # Evaluate model\n        val_loss, val_accuracy = model.evaluate(val_generator)\n        print(f\"\\nValidation Accuracy: {val_accuracy}\")\n\n        # Save model\n        model.save('weather_classification_model.h5')\n        \n        return model\n\n# Main execution\nif __name__ == \"__main__\":\n    # Create output directory for sample dataset\n    output_path = '/kaggle/working/bdd100k_sample'\n    os.makedirs(output_path, exist_ok=True)\n\n    # Initialize and run pipeline\n    pipeline = WeatherClassificationPipeline(\n        dataset_path='/kaggle/input/bdd100k-weather-classification',\n        output_path=output_path\n    )\n    pipeline.run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:25:11.336290Z","iopub.execute_input":"2024-12-17T18:25:11.336937Z","iopub.status.idle":"2024-12-17T22:54:13.008453Z","shell.execute_reply.started":"2024-12-17T18:25:11.336879Z","shell.execute_reply":"2024-12-17T22:54:13.007745Z"}},"outputs":[{"name":"stdout","text":"\n--- STEP: Extracting Sample Dataset ---\nClass clear:\n  Train images: 4000\n  Validation images: 1000\nClass overcast:\n  Train images: 4000\n  Validation images: 1000\nClass partly cloudy:\n  Train images: 4000\n  Validation images: 881\nClass rainy:\n  Train images: 4000\n  Validation images: 1000\nClass snowy:\n  Train images: 4000\n  Validation images: 1000\nClass unknown:\n  Train images: 4000\n  Validation images: 1000\n\n--- STEP: Visualizing Sample Images ---\n\n--- STEP: Preparing Data Generator ---\nFound 24000 images belonging to 6 classes.\nFound 5881 images belonging to 6 classes.\nClasses: ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\nNumber of training samples: 24000\nNumber of validation samples: 5881\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1734460239.338342     105 service.cc:145] XLA service 0x7e03d4108620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1734460239.338412     105 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1734460239.338417     105 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  2/750\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 76ms/step - accuracy: 0.2031 - loss: 1.9970   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1734460249.452922     105 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 445ms/step - accuracy: 0.3699 - loss: 1.5312 - val_accuracy: 0.2836 - val_loss: 2.0467 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 462ms/step - accuracy: 0.4461 - loss: 1.3537 - val_accuracy: 0.2998 - val_loss: 1.8152 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 444ms/step - accuracy: 0.4737 - loss: 1.3055 - val_accuracy: 0.4574 - val_loss: 1.2980 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 437ms/step - accuracy: 0.4926 - loss: 1.2671 - val_accuracy: 0.3328 - val_loss: 1.5561 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 471ms/step - accuracy: 0.5043 - loss: 1.2268 - val_accuracy: 0.3940 - val_loss: 1.5229 - learning_rate: 0.0010\nEpoch 6/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 468ms/step - accuracy: 0.5406 - loss: 1.1483 - val_accuracy: 0.4594 - val_loss: 1.3213 - learning_rate: 0.0010\nEpoch 7/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 471ms/step - accuracy: 0.5667 - loss: 1.1203 - val_accuracy: 0.5586 - val_loss: 1.1538 - learning_rate: 0.0010\nEpoch 8/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 452ms/step - accuracy: 0.5845 - loss: 1.0800 - val_accuracy: 0.5462 - val_loss: 1.1702 - learning_rate: 0.0010\nEpoch 9/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.6123 - loss: 1.0168 - val_accuracy: 0.3576 - val_loss: 1.8820 - learning_rate: 0.0010\nEpoch 10/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 422ms/step - accuracy: 0.6276 - loss: 0.9917 - val_accuracy: 0.6004 - val_loss: 1.0564 - learning_rate: 0.0010\nEpoch 11/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 426ms/step - accuracy: 0.6322 - loss: 0.9707 - val_accuracy: 0.6499 - val_loss: 0.9243 - learning_rate: 0.0010\nEpoch 12/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 426ms/step - accuracy: 0.6430 - loss: 0.9476 - val_accuracy: 0.5664 - val_loss: 1.1611 - learning_rate: 0.0010\nEpoch 13/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.6553 - loss: 0.9177 - val_accuracy: 0.6528 - val_loss: 0.9387 - learning_rate: 0.0010\nEpoch 14/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 427ms/step - accuracy: 0.6591 - loss: 0.9093 - val_accuracy: 0.6048 - val_loss: 1.0987 - learning_rate: 0.0010\nEpoch 15/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 425ms/step - accuracy: 0.6671 - loss: 0.8914 - val_accuracy: 0.6229 - val_loss: 1.0144 - learning_rate: 0.0010\nEpoch 16/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 425ms/step - accuracy: 0.6714 - loss: 0.8804 - val_accuracy: 0.5542 - val_loss: 1.2075 - learning_rate: 0.0010\nEpoch 17/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 437ms/step - accuracy: 0.6973 - loss: 0.8100 - val_accuracy: 0.6977 - val_loss: 0.8092 - learning_rate: 2.0000e-04\nEpoch 18/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.7075 - loss: 0.7989 - val_accuracy: 0.6934 - val_loss: 0.8323 - learning_rate: 2.0000e-04\nEpoch 19/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 434ms/step - accuracy: 0.7099 - loss: 0.7918 - val_accuracy: 0.7029 - val_loss: 0.8054 - learning_rate: 2.0000e-04\nEpoch 20/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.7137 - loss: 0.7795 - val_accuracy: 0.7029 - val_loss: 0.8090 - learning_rate: 2.0000e-04\nEpoch 21/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 430ms/step - accuracy: 0.7140 - loss: 0.7670 - val_accuracy: 0.7137 - val_loss: 0.7831 - learning_rate: 2.0000e-04\nEpoch 22/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.7174 - loss: 0.7545 - val_accuracy: 0.7137 - val_loss: 0.7762 - learning_rate: 2.0000e-04\nEpoch 23/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.7156 - loss: 0.7635 - val_accuracy: 0.7096 - val_loss: 0.7886 - learning_rate: 2.0000e-04\nEpoch 24/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.7176 - loss: 0.7651 - val_accuracy: 0.6997 - val_loss: 0.8122 - learning_rate: 2.0000e-04\nEpoch 25/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 422ms/step - accuracy: 0.7217 - loss: 0.7470 - val_accuracy: 0.7087 - val_loss: 0.7924 - learning_rate: 2.0000e-04\nEpoch 26/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 426ms/step - accuracy: 0.7247 - loss: 0.7524 - val_accuracy: 0.7070 - val_loss: 0.7985 - learning_rate: 2.0000e-04\nEpoch 27/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.7262 - loss: 0.7395 - val_accuracy: 0.7058 - val_loss: 0.8043 - learning_rate: 2.0000e-04\nEpoch 28/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.7333 - loss: 0.7248 - val_accuracy: 0.7201 - val_loss: 0.7644 - learning_rate: 4.0000e-05\nEpoch 29/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.7313 - loss: 0.7278 - val_accuracy: 0.7223 - val_loss: 0.7599 - learning_rate: 4.0000e-05\nEpoch 30/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.7318 - loss: 0.7202 - val_accuracy: 0.7196 - val_loss: 0.7635 - learning_rate: 4.0000e-05\nEpoch 31/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 424ms/step - accuracy: 0.7351 - loss: 0.7127 - val_accuracy: 0.7227 - val_loss: 0.7648 - learning_rate: 4.0000e-05\nEpoch 32/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 426ms/step - accuracy: 0.7377 - loss: 0.7133 - val_accuracy: 0.7198 - val_loss: 0.7706 - learning_rate: 4.0000e-05\nEpoch 33/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.7331 - loss: 0.7141 - val_accuracy: 0.7203 - val_loss: 0.7622 - learning_rate: 4.0000e-05\nEpoch 34/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 426ms/step - accuracy: 0.7360 - loss: 0.7091 - val_accuracy: 0.7184 - val_loss: 0.7662 - learning_rate: 4.0000e-05\nEpoch 35/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 462ms/step - accuracy: 0.7379 - loss: 0.7088 - val_accuracy: 0.7237 - val_loss: 0.7622 - learning_rate: 1.0000e-05\nEpoch 36/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 465ms/step - accuracy: 0.7385 - loss: 0.7058 - val_accuracy: 0.7247 - val_loss: 0.7593 - learning_rate: 1.0000e-05\nEpoch 37/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 449ms/step - accuracy: 0.7414 - loss: 0.7081 - val_accuracy: 0.7233 - val_loss: 0.7577 - learning_rate: 1.0000e-05\nEpoch 38/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 445ms/step - accuracy: 0.7332 - loss: 0.7221 - val_accuracy: 0.7232 - val_loss: 0.7601 - learning_rate: 1.0000e-05\nEpoch 39/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 442ms/step - accuracy: 0.7371 - loss: 0.7081 - val_accuracy: 0.7225 - val_loss: 0.7616 - learning_rate: 1.0000e-05\nEpoch 40/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 441ms/step - accuracy: 0.7370 - loss: 0.6992 - val_accuracy: 0.7232 - val_loss: 0.7580 - learning_rate: 1.0000e-05\nEpoch 41/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 437ms/step - accuracy: 0.7431 - loss: 0.7008 - val_accuracy: 0.7242 - val_loss: 0.7581 - learning_rate: 1.0000e-05\nEpoch 42/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 440ms/step - accuracy: 0.7347 - loss: 0.7157 - val_accuracy: 0.7235 - val_loss: 0.7607 - learning_rate: 1.0000e-05\nEpoch 43/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.7391 - loss: 0.6998 - val_accuracy: 0.7230 - val_loss: 0.7625 - learning_rate: 1.0000e-05\nEpoch 44/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 498ms/step - accuracy: 0.7335 - loss: 0.7057 - val_accuracy: 0.7235 - val_loss: 0.7583 - learning_rate: 1.0000e-05\nEpoch 45/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 483ms/step - accuracy: 0.7370 - loss: 0.7129 - val_accuracy: 0.7232 - val_loss: 0.7594 - learning_rate: 1.0000e-05\nEpoch 46/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 468ms/step - accuracy: 0.7385 - loss: 0.7022 - val_accuracy: 0.7249 - val_loss: 0.7601 - learning_rate: 1.0000e-05\nEpoch 47/50\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 473ms/step - accuracy: 0.7423 - loss: 0.6977 - val_accuracy: 0.7203 - val_loss: 0.7640 - learning_rate: 1.0000e-05\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 135ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 146ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 144ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 134ms/step\n\n--- STEP: Predicting Single Image ---\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\nPredicted Class: clear\nConfidence: 61.62%\nclear: 61.62%\novercast: 24.35%\npartly cloudy: 8.12%\nrainy: 0.84%\nsnowy: 0.05%\nunknown: 5.01%\n\nClassification Report Summary:\nclear:\n  Precision: 0.7065\n  Recall: 0.7680\n  F1-Score: 0.7360\novercast:\n  Precision: 0.6880\n  Recall: 0.5580\n  F1-Score: 0.6162\npartly cloudy:\n  Precision: 0.7213\n  Recall: 0.8048\n  F1-Score: 0.7607\nrainy:\n  Precision: 0.7932\n  Recall: 0.6790\n  F1-Score: 0.7317\nsnowy:\n  Precision: 0.7590\n  Recall: 0.7560\n  F1-Score: 0.7575\nunknown:\n  Precision: 0.6829\n  Recall: 0.7840\n  F1-Score: 0.7300\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 140ms/step - accuracy: 0.7121 - loss: 0.7685\n\nValidation Accuracy: 0.7233463525772095\n","output_type":"stream"}],"execution_count":1}]}