{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7209801,"sourceType":"datasetVersion","datasetId":4171601}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau, \n    EarlyStopping, \n    ModelCheckpoint\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    roc_curve, \n    auc, \n    precision_recall_curve, \n    average_precision_score\n)\nimport shutil\n\n# Memory and GPU configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass WeatherClassificationPipeline:\n    def __init__(self, dataset_path, output_path, target_size=(224, 224), sample_size=30000):\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.target_size = target_size\n        self.sample_size = sample_size\n        \n        # Predefined classes to ensure consistency\n        self.classes = ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\n\n    def extract_sample_dataset(self):\n        \"\"\"Extract a stratified sample of images for train and validation\"\"\"\n        print(\"\\n--- STEP: Extracting Sample Dataset ---\")\n        \n        # Create output directories\n        sample_train_path = os.path.join(self.output_path, 'train')\n        sample_val_path = os.path.join(self.output_path, 'val')\n        os.makedirs(sample_train_path, exist_ok=True)\n        os.makedirs(sample_val_path, exist_ok=True)\n        \n        # Calculate total samples and samples per class for train and val\n        train_sample_ratio = 0.8\n        val_sample_ratio = 0.2\n        total_samples_per_class = self.sample_size // len(self.classes)\n        train_samples_per_class = int(total_samples_per_class * train_sample_ratio)\n        val_samples_per_class = int(total_samples_per_class * val_sample_ratio)\n\n        for cls in self.classes:\n            # Create class directories\n            train_cls_path = os.path.join(sample_train_path, cls)\n            val_cls_path = os.path.join(sample_val_path, cls)\n            os.makedirs(train_cls_path, exist_ok=True)\n            os.makedirs(val_cls_path, exist_ok=True)\n\n            # Find source directory\n            cls_source_path = os.path.join(self.dataset_path, 'train', cls)\n            \n            # Get all image files\n            image_files = [f for f in os.listdir(cls_source_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Randomly shuffle images\n            np.random.shuffle(image_files)\n            \n            # Select train and validation samples\n            train_images = image_files[:train_samples_per_class]\n            val_images = image_files[train_samples_per_class:train_samples_per_class+val_samples_per_class]\n            \n            # Copy train images\n            for img in train_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(train_cls_path, img)\n                shutil.copy(src, dst)\n            \n            # Copy validation images\n            for img in val_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(val_cls_path, img)\n                shutil.copy(src, dst)\n            \n            print(f\"Class {cls}:\")\n            print(f\"  Train images: {len(train_images)}\")\n            print(f\"  Validation images: {len(val_images)}\")\n\n        return self.output_path\n\n    def prepare_data_generators(self, dataset_path):\n        \"\"\"Prepare data generators for training and validation\"\"\"\n        print(\"\\n--- STEP: Preparing Data Generator ---\")\n        \n        # Create ImageDataGenerator with preprocessing and augmentation\n        train_datagen = ImageDataGenerator(\n            preprocessing_function=preprocess_input,\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True\n        )\n\n        val_datagen = ImageDataGenerator(\n            preprocessing_function=preprocess_input\n        )\n\n        # Train generator\n        train_generator = train_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'train'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=True,\n            classes=self.classes\n        )\n\n        # Validation generator\n        val_generator = val_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'val'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=False,\n            classes=self.classes\n        )\n        \n        print(f\"Classes: {self.classes}\")\n        print(f\"Number of training samples: {train_generator.samples}\")\n        print(f\"Number of validation samples: {val_generator.samples}\")\n\n        return train_generator, val_generator\n\n    def create_densenet_transfer_model(self):\n        \"\"\"Create DenseNet transfer learning model\"\"\"\n        print(\"\\n--- STEP: Creating Transfer Learning Model ---\")\n        # Load pre-trained DenseNet121 model\n        base_model = DenseNet121(\n            weights='imagenet', \n            include_top=False, \n            input_shape=(*self.target_size, 3)\n        )\n        \n        # Freeze base model layers\n        base_model.trainable = False\n        \n        # Add custom classification layers\n        x = base_model.output\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(1024, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.5)(x)\n        x = layers.Dense(512, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.3)(x)\n        outputs = layers.Dense(len(self.classes), activation='softmax')(x)\n        \n        model = models.Model(inputs=base_model.input, outputs=outputs)\n        return model\n\n    def fine_tune_model(self, model, train_generator, val_generator):\n        \"\"\"Apply fine-tuning strategy\"\"\"\n        print(\"\\n--- STEP: Fine-Tuning Model ---\")\n        # Unfreeze last few layers of base model for fine-tuning\n        for layer in model.layers[-80:]:\n            layer.trainable = True\n        \n        # Compile with lower learning rate\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=1e-5),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Fine-tuning callbacks\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.5, \n            patience=3, \n            min_lr=1e-6,\n            verbose=1\n        )\n        \n        early_stopping = EarlyStopping(\n            monitor='val_accuracy', \n            patience=3, \n            restore_best_weights=True,\n            verbose=1\n        )\n        \n        # Fine-tuning training\n        fine_tune_history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=30,\n            callbacks=[reduce_lr, early_stopping]\n        )\n        \n        return fine_tune_history\n\n    def plot_training_curves(self, history):\n        \"\"\"Plot training and validation accuracy/loss\"\"\"\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(history['accuracy'], label='Training Accuracy')\n        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Model Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(history['loss'], label='Training Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.title('Model Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig('training_curves.png')\n        plt.close()\n\n    def plot_confusion_matrix(self, val_generator, model):\n        \"\"\"Plot confusion matrix\"\"\"\n        # Get predictions\n        predictions = model.predict(val_generator)\n        y_pred = np.argmax(predictions, axis=1)\n        y_true = val_generator.classes\n\n        # Plot confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', \n                    xticklabels=self.classes, \n                    yticklabels=self.classes)\n        plt.title('Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.tight_layout()\n        plt.savefig('confusion_matrix.png')\n        plt.close()\n\n    def plot_roc_curve(self, val_generator, model):\n        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute ROC curve and ROC area for each class\n        n_classes = len(self.classes)\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        \n        # Binarize the output\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n            plt.plot(fpr[i], tpr[i], \n                     label=f'ROC curve (class: {self.classes[i]}, area = {roc_auc[i]:.2f})')\n        \n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend(loc=\"lower right\")\n        plt.tight_layout()\n        plt.savefig('roc_curve.png')\n        plt.close()\n\n    def plot_precision_recall_curve(self, val_generator, model):\n        \"\"\"Plot Precision-Recall curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute Precision-Recall curve\n        n_classes = len(self.classes)\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            avg_precision = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n            \n            plt.plot(recall, precision, \n                     label=f'Precision-Recall curve (class: {self.classes[i]}, AP = {avg_precision:.2f})')\n        \n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curve')\n        plt.legend(loc=\"best\")\n        plt.tight_layout()\n        plt.savefig('precision_recall_curve.png')\n        plt.close()\n\n    def generate_classification_report(self, val_generator, model):\n        \"\"\"Generate and save detailed classification report\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        y_true = val_generator.classes\n        \n        # Generate classification report\n        report = classification_report(\n            y_true, \n            y_pred, \n            target_names=self.classes, \n            output_dict=True\n        )\n        \n        # Create a DataFrame for better visualization\n        report_df = pd.DataFrame(report).transpose()\n        \n        # Plot as heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(report_df.iloc[:-2, :-1].astype(float), \n                    annot=True, \n                    cmap='YlGnBu', \n                    fmt='.2f')\n        plt.title('Classification Report Metrics')\n        plt.tight_layout()\n        plt.savefig('classification_report_heatmap.png')\n        plt.close()\n        \n        # Save textual report\n        with open('classification_report.txt', 'w') as f:\n            f.write(classification_report(\n                y_true, \n                y_pred, \n                target_names=self.classes\n            ))\n        \n        return report  \n\n    def visualize_sample_images(self, dataset_path):\n        \"\"\"Visualize sample images from each class\"\"\"\n        print(\"\\n--- STEP: Visualizing Sample Images ---\")\n        \n        # Create figure for sample images\n        plt.figure(figsize=(15, 10))\n        \n        # Get train directory\n        train_dir = os.path.join(dataset_path, 'train')\n        \n        # Iterate through classes\n        for i, cls in enumerate(self.classes, 1):\n            # Get path to class directory\n            cls_path = os.path.join(train_dir, cls)\n            \n            # Get list of image files\n            image_files = [f for f in os.listdir(cls_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Select first image\n            if image_files:\n                img_path = os.path.join(cls_path, image_files[0])\n                \n                # Load and display image\n                plt.subplot(2, 3, i)\n                img = plt.imread(img_path)\n                plt.imshow(img)\n                plt.title(cls)\n                plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('sample_images.png')\n        plt.close()\n\n    def predict_single_image(self, model, image_path):\n        \"\"\"Predict weather class for a single image\"\"\"\n        print(\"\\n--- STEP: Predicting Single Image ---\")\n        \n        # Load and preprocess the image\n        img = load_img(image_path, target_size=self.target_size)\n        img_array = img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0)\n        img_array = preprocess_input(img_array)\n        \n        # Make prediction\n        predictions = model.predict(img_array)\n        predicted_class_index = np.argmax(predictions[0])\n        predicted_class = self.classes[predicted_class_index]\n        confidence = predictions[0][predicted_class_index]\n        \n        # Visualize prediction\n        plt.figure(figsize=(10, 5))\n        \n        # Original image\n        plt.subplot(1, 2, 1)\n        plt.imshow(plt.imread(image_path))\n        plt.title('Original Image')\n        plt.axis('off')\n        \n        # Prediction bar plot\n        plt.subplot(1, 2, 2)\n        plt.bar(self.classes, predictions[0])\n        plt.title('Class Probabilities')\n        plt.xlabel('Weather Classes')\n        plt.ylabel('Probability')\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig('single_image_prediction.png')\n        plt.close()\n        \n        # Print prediction details\n        print(f\"Predicted Class: {predicted_class}\")\n        print(f\"Confidence: {confidence:.2%}\")\n        \n        # Print full probabilities\n        for cls, prob in zip(self.classes, predictions[0]):\n            print(f\"{cls}: {prob:.2%}\")\n        \n        return predicted_class, confidence\n           \n    def run_pipeline(self):\n        # Extract sample dataset\n        sample_train_path = self.extract_sample_dataset()\n        \n        # Visualize sample images\n        self.visualize_sample_images(sample_train_path)\n\n        # Prepare data generators\n        train_generator, val_generator = self.prepare_data_generators(sample_train_path)\n\n        # Create model\n        model = self.create_densenet_transfer_model()\n            \n        # Compile model\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=0.005),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n            \n        # Initial training\n        initial_history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=20\n        )\n\n        # Fine-tune the model\n        fine_tune_history = self.fine_tune_model(model, train_generator, val_generator)\n\n        # Combine training histories\n        combined_history = {\n            'accuracy': initial_history.history['accuracy'] + fine_tune_history.history['accuracy'],\n            'val_accuracy': initial_history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'],\n            'loss': initial_history.history['loss'] + fine_tune_history.history['loss'],\n            'val_loss': initial_history.history['val_loss'] + fine_tune_history.history['val_loss']\n        }\n        \n        # Visualizations with combined history\n        self.plot_training_curves(combined_history)\n        self.plot_confusion_matrix(val_generator, model)\n        self.plot_roc_curve(val_generator, model)\n        self.plot_precision_recall_curve(val_generator, model)\n        report = self.generate_classification_report(val_generator, model)\n            \n        # Predict single image \n        test_image_path = '/kaggle/input/bdd100k-weather-classification/test/cabc30fc-e7726578.jpg'\n        self.predict_single_image(model, test_image_path)\n            \n        # Print out some key metrics from the report\n        print(\"\\nClassification Report Summary:\")\n        for cls in self.classes:\n            print(f\"{cls}:\")\n            print(f\"  Precision: {report[cls]['precision']:.4f}\")\n            print(f\"  Recall: {report[cls]['recall']:.4f}\")\n            print(f\"  F1-Score: {report[cls]['f1-score']:.4f}\")\n            \n        # Evaluate model\n        val_loss, val_accuracy = model.evaluate(val_generator)\n        print(f\"\\nValidation Accuracy: {val_accuracy}\")\n            \n        # Save model\n        model.save('weather_classification_densenet_model.h5')\n            \n        return model\n\n# Main execution\nif __name__ == \"__main__\":\n    # Create output directory for sample dataset\n    output_path = '/kaggle/working/bdd100k_sample'\n    os.makedirs(output_path, exist_ok=True)\n\n    # Initialize and run pipeline\n    pipeline = WeatherClassificationPipeline(\n        dataset_path='/kaggle/input/bdd100k-weather-classification',\n        output_path=output_path\n    )\n    pipeline.run_pipeline()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T20:32:09.444240Z","iopub.execute_input":"2024-12-18T20:32:09.444483Z","iopub.status.idle":"2024-12-19T01:12:49.679845Z","shell.execute_reply.started":"2024-12-18T20:32:09.444464Z","shell.execute_reply":"2024-12-19T01:12:49.679071Z"}},"outputs":[{"name":"stdout","text":"\n--- STEP: Extracting Sample Dataset ---\nClass clear:\n  Train images: 4000\n  Validation images: 1000\nClass overcast:\n  Train images: 4000\n  Validation images: 1000\nClass partly cloudy:\n  Train images: 4000\n  Validation images: 881\nClass rainy:\n  Train images: 4000\n  Validation images: 1000\nClass snowy:\n  Train images: 4000\n  Validation images: 1000\nClass unknown:\n  Train images: 4000\n  Validation images: 1000\n\n--- STEP: Visualizing Sample Images ---\n\n--- STEP: Preparing Data Generator ---\nFound 24000 images belonging to 6 classes.\nFound 5881 images belonging to 6 classes.\nClasses: ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\nNumber of training samples: 24000\nNumber of validation samples: 5881\n\n--- STEP: Creating Transfer Learning Model ---\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 464ms/step - accuracy: 0.4471 - loss: 1.6556 - val_accuracy: 0.5638 - val_loss: 1.2188\nEpoch 2/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 438ms/step - accuracy: 0.5499 - loss: 1.1777 - val_accuracy: 0.5448 - val_loss: 1.1588\nEpoch 3/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 429ms/step - accuracy: 0.5675 - loss: 1.1157 - val_accuracy: 0.5967 - val_loss: 1.0613\nEpoch 4/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5837 - loss: 1.0845 - val_accuracy: 0.5931 - val_loss: 1.0607\nEpoch 5/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 429ms/step - accuracy: 0.5859 - loss: 1.0804 - val_accuracy: 0.5854 - val_loss: 1.0916\nEpoch 6/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 425ms/step - accuracy: 0.5844 - loss: 1.0777 - val_accuracy: 0.5746 - val_loss: 1.1003\nEpoch 7/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.5902 - loss: 1.0679 - val_accuracy: 0.5934 - val_loss: 1.0587\nEpoch 8/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.5938 - loss: 1.0582 - val_accuracy: 0.6031 - val_loss: 1.0410\nEpoch 9/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.5956 - loss: 1.0532 - val_accuracy: 0.5718 - val_loss: 1.1117\nEpoch 10/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 433ms/step - accuracy: 0.5943 - loss: 1.0519 - val_accuracy: 0.6035 - val_loss: 1.0449\nEpoch 11/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 430ms/step - accuracy: 0.5945 - loss: 1.0567 - val_accuracy: 0.6024 - val_loss: 1.0614\nEpoch 12/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5879 - loss: 1.0648 - val_accuracy: 0.6157 - val_loss: 0.9987\nEpoch 13/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 430ms/step - accuracy: 0.5980 - loss: 1.0513 - val_accuracy: 0.6155 - val_loss: 0.9886\nEpoch 14/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.6070 - loss: 1.0316 - val_accuracy: 0.6077 - val_loss: 1.0209\nEpoch 15/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 434ms/step - accuracy: 0.6048 - loss: 1.0391 - val_accuracy: 0.6171 - val_loss: 1.0130\nEpoch 16/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.5953 - loss: 1.0477 - val_accuracy: 0.6036 - val_loss: 1.0142\nEpoch 17/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6032 - loss: 1.0456 - val_accuracy: 0.6142 - val_loss: 1.0061\nEpoch 18/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5980 - loss: 1.0510 - val_accuracy: 0.6031 - val_loss: 1.0617\nEpoch 19/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 436ms/step - accuracy: 0.5984 - loss: 1.0421 - val_accuracy: 0.6186 - val_loss: 1.0089\nEpoch 20/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 432ms/step - accuracy: 0.6100 - loss: 1.0276 - val_accuracy: 0.5963 - val_loss: 1.0461\n\n--- STEP: Fine-Tuning Model ---\nEpoch 1/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 453ms/step - accuracy: 0.5905 - loss: 1.0858 - val_accuracy: 0.6181 - val_loss: 1.0628 - learning_rate: 1.0000e-05\nEpoch 2/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 433ms/step - accuracy: 0.6167 - loss: 1.0274 - val_accuracy: 0.6291 - val_loss: 1.0624 - learning_rate: 1.0000e-05\nEpoch 3/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 429ms/step - accuracy: 0.6282 - loss: 0.9895 - val_accuracy: 0.6404 - val_loss: 1.0050 - learning_rate: 1.0000e-05\nEpoch 4/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 433ms/step - accuracy: 0.6244 - loss: 0.9843 - val_accuracy: 0.6446 - val_loss: 0.9560 - learning_rate: 1.0000e-05\nEpoch 5/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.6401 - loss: 0.9611 - val_accuracy: 0.6516 - val_loss: 1.0177 - learning_rate: 1.0000e-05\nEpoch 6/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 436ms/step - accuracy: 0.6398 - loss: 0.9463 - val_accuracy: 0.6598 - val_loss: 0.9713 - learning_rate: 1.0000e-05\nEpoch 7/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.6538 - loss: 0.9209\nEpoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 433ms/step - accuracy: 0.6537 - loss: 0.9209 - val_accuracy: 0.6632 - val_loss: 1.0338 - learning_rate: 1.0000e-05\nEpoch 8/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.6535 - loss: 0.9194 - val_accuracy: 0.6652 - val_loss: 0.9712 - learning_rate: 5.0000e-06\nEpoch 9/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 431ms/step - accuracy: 0.6579 - loss: 0.9203 - val_accuracy: 0.6662 - val_loss: 1.0010 - learning_rate: 5.0000e-06\nEpoch 10/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6607 - loss: 0.8987 - val_accuracy: 0.6679 - val_loss: 0.9510 - learning_rate: 5.0000e-06\nEpoch 11/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 429ms/step - accuracy: 0.6680 - loss: 0.9025 - val_accuracy: 0.6683 - val_loss: 0.9586 - learning_rate: 5.0000e-06\nEpoch 12/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.6664 - loss: 0.8980 - val_accuracy: 0.6710 - val_loss: 0.9442 - learning_rate: 5.0000e-06\nEpoch 13/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 430ms/step - accuracy: 0.6666 - loss: 0.8983 - val_accuracy: 0.6742 - val_loss: 0.9205 - learning_rate: 5.0000e-06\nEpoch 14/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6620 - loss: 0.8995 - val_accuracy: 0.6768 - val_loss: 0.9834 - learning_rate: 5.0000e-06\nEpoch 15/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6645 - loss: 0.8922 - val_accuracy: 0.6744 - val_loss: 0.9552 - learning_rate: 5.0000e-06\nEpoch 16/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.6761 - loss: 0.8815\nEpoch 16: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.6761 - loss: 0.8815 - val_accuracy: 0.6793 - val_loss: 0.9389 - learning_rate: 5.0000e-06\nEpoch 17/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6746 - loss: 0.8728 - val_accuracy: 0.6796 - val_loss: 0.9238 - learning_rate: 2.5000e-06\nEpoch 18/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 430ms/step - accuracy: 0.6726 - loss: 0.8824 - val_accuracy: 0.6793 - val_loss: 0.9414 - learning_rate: 2.5000e-06\nEpoch 19/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 0.6756 - loss: 0.8694\nEpoch 19: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 430ms/step - accuracy: 0.6756 - loss: 0.8695 - val_accuracy: 0.6796 - val_loss: 0.9555 - learning_rate: 2.5000e-06\nEpoch 20/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 436ms/step - accuracy: 0.6660 - loss: 0.8826 - val_accuracy: 0.6812 - val_loss: 0.9535 - learning_rate: 1.2500e-06\nEpoch 21/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.6784 - loss: 0.8695 - val_accuracy: 0.6786 - val_loss: 0.9039 - learning_rate: 1.2500e-06\nEpoch 22/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 436ms/step - accuracy: 0.6806 - loss: 0.8617 - val_accuracy: 0.6807 - val_loss: 0.9299 - learning_rate: 1.2500e-06\nEpoch 23/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 430ms/step - accuracy: 0.6853 - loss: 0.8564 - val_accuracy: 0.6815 - val_loss: 0.9105 - learning_rate: 1.2500e-06\nEpoch 24/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 0.6818 - loss: 0.8576\nEpoch 24: ReduceLROnPlateau reducing learning rate to 1e-06.\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6818 - loss: 0.8576 - val_accuracy: 0.6819 - val_loss: 0.9253 - learning_rate: 1.2500e-06\nEpoch 25/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 434ms/step - accuracy: 0.6741 - loss: 0.8710 - val_accuracy: 0.6824 - val_loss: 0.9162 - learning_rate: 1.0000e-06\nEpoch 26/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.6777 - loss: 0.8629 - val_accuracy: 0.6817 - val_loss: 0.9198 - learning_rate: 1.0000e-06\nEpoch 27/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 442ms/step - accuracy: 0.6798 - loss: 0.8650 - val_accuracy: 0.6825 - val_loss: 0.9475 - learning_rate: 1.0000e-06\nEpoch 28/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 434ms/step - accuracy: 0.6760 - loss: 0.8668 - val_accuracy: 0.6810 - val_loss: 0.9096 - learning_rate: 1.0000e-06\nEpoch 29/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 431ms/step - accuracy: 0.6810 - loss: 0.8616 - val_accuracy: 0.6813 - val_loss: 0.9274 - learning_rate: 1.0000e-06\nEpoch 30/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 437ms/step - accuracy: 0.6744 - loss: 0.8755 - val_accuracy: 0.6803 - val_loss: 0.8804 - learning_rate: 1.0000e-06\nEpoch 30: early stopping\nRestoring model weights from the end of the best epoch: 27.\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 138ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 136ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 139ms/step\n\n--- STEP: Predicting Single Image ---\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step\nPredicted Class: partly cloudy\nConfidence: 34.61%\nclear: 25.53%\novercast: 34.03%\npartly cloudy: 34.61%\nrainy: 1.62%\nsnowy: 0.21%\nunknown: 4.00%\n\nClassification Report Summary:\nclear:\n  Precision: 0.6639\n  Recall: 0.7270\n  F1-Score: 0.6940\novercast:\n  Precision: 0.5812\n  Recall: 0.5690\n  F1-Score: 0.5750\npartly cloudy:\n  Precision: 0.6037\n  Recall: 0.7333\n  F1-Score: 0.6622\nrainy:\n  Precision: 0.8378\n  Recall: 0.6560\n  F1-Score: 0.7358\nsnowy:\n  Precision: 0.7790\n  Recall: 0.7260\n  F1-Score: 0.7516\nunknown:\n  Precision: 0.6751\n  Recall: 0.6900\n  F1-Score: 0.6825\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 132ms/step - accuracy: 0.6789 - loss: 0.8690\n\nValidation Accuracy: 0.6825369596481323\n","output_type":"stream"}],"execution_count":1}]}