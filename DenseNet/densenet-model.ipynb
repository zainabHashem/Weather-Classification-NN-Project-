{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7209801,"sourceType":"datasetVersion","datasetId":4171601}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau, \n    EarlyStopping, \n    ModelCheckpoint\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    roc_curve, \n    auc, \n    precision_recall_curve, \n    average_precision_score\n)\nimport shutil\n\n# Memory and GPU configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass WeatherClassificationPipeline:\n    def __init__(self, dataset_path, output_path, target_size=(224, 224), sample_size=30000):\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.target_size = target_size\n        self.sample_size = sample_size\n        \n        # Predefined classes to ensure consistency\n        self.classes = ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\n\n    def extract_sample_dataset(self):\n        \"\"\"Extract a stratified sample of images for train and validation\"\"\"\n        print(\"\\n--- STEP: Extracting Sample Dataset ---\")\n        \n        # Create output directories\n        sample_train_path = os.path.join(self.output_path, 'train')\n        sample_val_path = os.path.join(self.output_path, 'val')\n        os.makedirs(sample_train_path, exist_ok=True)\n        os.makedirs(sample_val_path, exist_ok=True)\n        \n        # Calculate total samples and samples per class for train and val\n        train_sample_ratio = 0.8\n        val_sample_ratio = 0.2\n        total_samples_per_class = self.sample_size // len(self.classes)\n        train_samples_per_class = int(total_samples_per_class * train_sample_ratio)\n        val_samples_per_class = int(total_samples_per_class * val_sample_ratio)\n\n        for cls in self.classes:\n            # Create class directories\n            train_cls_path = os.path.join(sample_train_path, cls)\n            val_cls_path = os.path.join(sample_val_path, cls)\n            os.makedirs(train_cls_path, exist_ok=True)\n            os.makedirs(val_cls_path, exist_ok=True)\n\n            # Find source directory\n            cls_source_path = os.path.join(self.dataset_path, 'train', cls)\n            \n            # Get all image files\n            image_files = [f for f in os.listdir(cls_source_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Randomly shuffle images\n            np.random.shuffle(image_files)\n            \n            # Select train and validation samples\n            train_images = image_files[:train_samples_per_class]\n            val_images = image_files[train_samples_per_class:train_samples_per_class+val_samples_per_class]\n            \n            # Copy train images\n            for img in train_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(train_cls_path, img)\n                shutil.copy(src, dst)\n            \n            # Copy validation images\n            for img in val_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(val_cls_path, img)\n                shutil.copy(src, dst)\n            \n            print(f\"Class {cls}:\")\n            print(f\"  Train images: {len(train_images)}\")\n            print(f\"  Validation images: {len(val_images)}\")\n\n        return self.output_path\n\n    def prepare_data_generators(self, dataset_path):\n        \"\"\"Prepare data generators for training and validation\"\"\"\n        print(\"\\n--- STEP: Preparing Data Generator ---\")\n        \n        # Create ImageDataGenerator with preprocessing and augmentation\n        train_datagen = ImageDataGenerator(\n            preprocessing_function=preprocess_input,\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True\n        )\n\n        val_datagen = ImageDataGenerator(\n            preprocessing_function=preprocess_input\n        )\n\n        # Train generator\n        train_generator = train_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'train'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=True,\n            classes=self.classes\n        )\n\n        # Validation generator\n        val_generator = val_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'val'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=False,\n            classes=self.classes\n        )\n        \n        print(f\"Classes: {self.classes}\")\n        print(f\"Number of training samples: {train_generator.samples}\")\n        print(f\"Number of validation samples: {val_generator.samples}\")\n\n        return train_generator, val_generator\n\n    def create_densenet_transfer_model(self):\n        \"\"\"Create DenseNet transfer learning model\"\"\"\n        print(\"\\n--- STEP: Creating Transfer Learning Model ---\")\n        # Load pre-trained DenseNet121 model\n        base_model = DenseNet121(\n            weights='imagenet', \n            include_top=False, \n            input_shape=(*self.target_size, 3)\n        )\n        \n        # Freeze base model layers\n        base_model.trainable = False\n        \n        # Add custom classification layers\n        x = base_model.output\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(1024, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.5)(x)\n        x = layers.Dense(512, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.3)(x)\n        outputs = layers.Dense(len(self.classes), activation='softmax')(x)\n        \n        model = models.Model(inputs=base_model.input, outputs=outputs)\n        return model\n\n    def fine_tune_model(self, model, train_generator, val_generator):\n        \"\"\"Apply fine-tuning strategy\"\"\"\n        print(\"\\n--- STEP: Fine-Tuning Model ---\")\n        # Unfreeze last few layers of base model for fine-tuning\n        for layer in model.layers[-80:]:\n            layer.trainable = True\n        \n        # Compile with lower learning rate\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=1e-5),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Fine-tuning callbacks\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.5, \n            patience=3, \n            min_lr=1e-6,\n            verbose=1\n        )\n        \n        early_stopping = EarlyStopping(\n            monitor='val_accuracy', \n            patience=3, \n            restore_best_weights=True,\n            verbose=1\n        )\n        \n        # Fine-tuning training\n        fine_tune_history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=80,\n            callbacks=[reduce_lr, early_stopping]\n        )\n        \n        return fine_tune_history\n\n    def plot_training_curves(self, history):\n        \"\"\"Plot training and validation accuracy/loss\"\"\"\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(history['accuracy'], label='Training Accuracy')\n        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Model Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(history['loss'], label='Training Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.title('Model Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig('training_curves.png')\n        plt.close()\n\n    def plot_confusion_matrix(self, val_generator, model):\n        \"\"\"Plot confusion matrix\"\"\"\n        # Get predictions\n        predictions = model.predict(val_generator)\n        y_pred = np.argmax(predictions, axis=1)\n        y_true = val_generator.classes\n\n        # Plot confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', \n                    xticklabels=self.classes, \n                    yticklabels=self.classes)\n        plt.title('Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.tight_layout()\n        plt.savefig('confusion_matrix.png')\n        plt.close()\n\n    def plot_roc_curve(self, val_generator, model):\n        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute ROC curve and ROC area for each class\n        n_classes = len(self.classes)\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        \n        # Binarize the output\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n            plt.plot(fpr[i], tpr[i], \n                     label=f'ROC curve (class: {self.classes[i]}, area = {roc_auc[i]:.2f})')\n        \n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend(loc=\"lower right\")\n        plt.tight_layout()\n        plt.savefig('roc_curve.png')\n        plt.close()\n\n    def plot_precision_recall_curve(self, val_generator, model):\n        \"\"\"Plot Precision-Recall curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute Precision-Recall curve\n        n_classes = len(self.classes)\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            avg_precision = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n            \n            plt.plot(recall, precision, \n                     label=f'Precision-Recall curve (class: {self.classes[i]}, AP = {avg_precision:.2f})')\n        \n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curve')\n        plt.legend(loc=\"best\")\n        plt.tight_layout()\n        plt.savefig('precision_recall_curve.png')\n        plt.close()\n\n    def generate_classification_report(self, val_generator, model):\n        \"\"\"Generate and save detailed classification report\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        y_true = val_generator.classes\n        \n        # Generate classification report\n        report = classification_report(\n            y_true, \n            y_pred, \n            target_names=self.classes, \n            output_dict=True\n        )\n        \n        # Create a DataFrame for better visualization\n        report_df = pd.DataFrame(report).transpose()\n        \n        # Plot as heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(report_df.iloc[:-2, :-1].astype(float), \n                    annot=True, \n                    cmap='YlGnBu', \n                    fmt='.2f')\n        plt.title('Classification Report Metrics')\n        plt.tight_layout()\n        plt.savefig('classification_report_heatmap.png')\n        plt.close()\n        \n        # Save textual report\n        with open('classification_report.txt', 'w') as f:\n            f.write(classification_report(\n                y_true, \n                y_pred, \n                target_names=self.classes\n            ))\n        \n        return report  \n\n    def visualize_sample_images(self, dataset_path):\n        \"\"\"Visualize sample images from each class\"\"\"\n        print(\"\\n--- STEP: Visualizing Sample Images ---\")\n        \n        # Create figure for sample images\n        plt.figure(figsize=(15, 10))\n        \n        # Get train directory\n        train_dir = os.path.join(dataset_path, 'train')\n        \n        # Iterate through classes\n        for i, cls in enumerate(self.classes, 1):\n            # Get path to class directory\n            cls_path = os.path.join(train_dir, cls)\n            \n            # Get list of image files\n            image_files = [f for f in os.listdir(cls_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Select first image\n            if image_files:\n                img_path = os.path.join(cls_path, image_files[0])\n                \n                # Load and display image\n                plt.subplot(2, 3, i)\n                img = plt.imread(img_path)\n                plt.imshow(img)\n                plt.title(cls)\n                plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('sample_images.png')\n        plt.close()\n\n    def predict_single_image(self, model, image_path):\n        \"\"\"Predict weather class for a single image\"\"\"\n        print(\"\\n--- STEP: Predicting Single Image ---\")\n        \n        # Load and preprocess the image\n        img = load_img(image_path, target_size=self.target_size)\n        img_array = img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0)\n        img_array = preprocess_input(img_array)\n        \n        # Make prediction\n        predictions = model.predict(img_array)\n        predicted_class_index = np.argmax(predictions[0])\n        predicted_class = self.classes[predicted_class_index]\n        confidence = predictions[0][predicted_class_index]\n        \n        # Visualize prediction\n        plt.figure(figsize=(10, 5))\n        \n        # Original image\n        plt.subplot(1, 2, 1)\n        plt.imshow(plt.imread(image_path))\n        plt.title('Original Image')\n        plt.axis('off')\n        \n        # Prediction bar plot\n        plt.subplot(1, 2, 2)\n        plt.bar(self.classes, predictions[0])\n        plt.title('Class Probabilities')\n        plt.xlabel('Weather Classes')\n        plt.ylabel('Probability')\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig('single_image_prediction.png')\n        plt.close()\n        \n        # Print prediction details\n        print(f\"Predicted Class: {predicted_class}\")\n        print(f\"Confidence: {confidence:.2%}\")\n        \n        # Print full probabilities\n        for cls, prob in zip(self.classes, predictions[0]):\n            print(f\"{cls}: {prob:.2%}\")\n        \n        return predicted_class, confidence\n           \n    def run_pipeline(self):\n        # Extract sample dataset\n        sample_train_path = self.extract_sample_dataset()\n        \n        # Visualize sample images\n        self.visualize_sample_images(sample_train_path)\n\n        # Prepare data generators\n        train_generator, val_generator = self.prepare_data_generators(sample_train_path)\n\n        # Create model\n        model = self.create_densenet_transfer_model()\n            \n        # Compile model\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=0.005),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n            \n        # Initial training\n        initial_history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=20\n        )\n\n        # Fine-tune the model\n        fine_tune_history = self.fine_tune_model(model, train_generator, val_generator)\n\n        # Combine training histories\n        combined_history = {\n            'accuracy': initial_history.history['accuracy'] + fine_tune_history.history['accuracy'],\n            'val_accuracy': initial_history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'],\n            'loss': initial_history.history['loss'] + fine_tune_history.history['loss'],\n            'val_loss': initial_history.history['val_loss'] + fine_tune_history.history['val_loss']\n        }\n        \n        # Visualizations with combined history\n        self.plot_training_curves(combined_history)\n        self.plot_confusion_matrix(val_generator, model)\n        self.plot_roc_curve(val_generator, model)\n        self.plot_precision_recall_curve(val_generator, model)\n        report = self.generate_classification_report(val_generator, model)\n            \n        # Predict single image \n        test_image_path = '/kaggle/input/bdd100k-weather-classification/test/cabc30fc-e7726578.jpg'\n        self.predict_single_image(model, test_image_path)\n            \n        # Print out some key metrics from the report\n        print(\"\\nClassification Report Summary:\")\n        for cls in self.classes:\n            print(f\"{cls}:\")\n            print(f\"  Precision: {report[cls]['precision']:.4f}\")\n            print(f\"  Recall: {report[cls]['recall']:.4f}\")\n            print(f\"  F1-Score: {report[cls]['f1-score']:.4f}\")\n            \n        # Evaluate model\n        val_loss, val_accuracy = model.evaluate(val_generator)\n        print(f\"\\nValidation Accuracy: {val_accuracy}\")\n            \n        # Save model\n        model.save('weather_classification_densenet_model.h5')\n            \n        return model\n\n# Main execution\nif __name__ == \"__main__\":\n    # Create output directory for sample dataset\n    output_path = '/kaggle/working/bdd100k_sample'\n    os.makedirs(output_path, exist_ok=True)\n\n    # Initialize and run pipeline\n    pipeline = WeatherClassificationPipeline(\n        dataset_path='/kaggle/input/bdd100k-weather-classification',\n        output_path=output_path\n    )\n    pipeline.run_pipeline()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T02:52:58.494639Z","iopub.execute_input":"2024-12-19T02:52:58.494859Z","iopub.status.idle":"2024-12-19T07:25:40.149028Z","shell.execute_reply.started":"2024-12-19T02:52:58.494840Z","shell.execute_reply":"2024-12-19T07:25:40.147970Z"}},"outputs":[{"name":"stdout","text":"\n--- STEP: Extracting Sample Dataset ---\nClass clear:\n  Train images: 4000\n  Validation images: 1000\nClass overcast:\n  Train images: 4000\n  Validation images: 1000\nClass partly cloudy:\n  Train images: 4000\n  Validation images: 881\nClass rainy:\n  Train images: 4000\n  Validation images: 1000\nClass snowy:\n  Train images: 4000\n  Validation images: 1000\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 462ms/step - accuracy: 0.4399 - loss: 1.6654 - val_accuracy: 0.5579 - val_loss: 1.1478\nEpoch 2/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.5496 - loss: 1.1802 - val_accuracy: 0.5922 - val_loss: 1.0793\nEpoch 3/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 437ms/step - accuracy: 0.5745 - loss: 1.0971 - val_accuracy: 0.5989 - val_loss: 1.0346\nEpoch 4/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5904 - loss: 1.0759 - val_accuracy: 0.6089 - val_loss: 1.0413\nEpoch 5/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.5909 - loss: 1.0742 - val_accuracy: 0.5948 - val_loss: 1.0508\nEpoch 6/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 429ms/step - accuracy: 0.5926 - loss: 1.0631 - val_accuracy: 0.6065 - val_loss: 1.0569\nEpoch 7/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 430ms/step - accuracy: 0.5942 - loss: 1.0652 - val_accuracy: 0.6040 - val_loss: 1.0221\nEpoch 8/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5957 - loss: 1.0604 - val_accuracy: 0.6116 - val_loss: 1.0061\nEpoch 9/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5935 - loss: 1.0563 - val_accuracy: 0.6060 - val_loss: 1.0196\nEpoch 10/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 434ms/step - accuracy: 0.6005 - loss: 1.0439 - val_accuracy: 0.6016 - val_loss: 1.0403\nEpoch 11/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.6000 - loss: 1.0549 - val_accuracy: 0.6229 - val_loss: 1.0123\nEpoch 12/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 437ms/step - accuracy: 0.5962 - loss: 1.0484 - val_accuracy: 0.6147 - val_loss: 1.0100\nEpoch 13/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 429ms/step - accuracy: 0.6006 - loss: 1.0435 - val_accuracy: 0.6179 - val_loss: 0.9953\nEpoch 14/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 433ms/step - accuracy: 0.6028 - loss: 1.0388 - val_accuracy: 0.6137 - val_loss: 0.9984\nEpoch 15/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 443ms/step - accuracy: 0.6038 - loss: 1.0351 - val_accuracy: 0.6145 - val_loss: 1.0176\nEpoch 16/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.5988 - loss: 1.0459 - val_accuracy: 0.5951 - val_loss: 1.0384\nEpoch 17/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 433ms/step - accuracy: 0.6030 - loss: 1.0370 - val_accuracy: 0.6133 - val_loss: 1.0261\nEpoch 18/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 426ms/step - accuracy: 0.6010 - loss: 1.0335 - val_accuracy: 0.6110 - val_loss: 1.0219\nEpoch 19/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 433ms/step - accuracy: 0.6110 - loss: 1.0232 - val_accuracy: 0.6196 - val_loss: 1.0024\nEpoch 20/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.6096 - loss: 1.0141 - val_accuracy: 0.6237 - val_loss: 0.9820\n\n--- STEP: Fine-Tuning Model ---\nEpoch 1/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 450ms/step - accuracy: 0.6067 - loss: 1.0366 - val_accuracy: 0.6263 - val_loss: 0.9978 - learning_rate: 1.0000e-05\nEpoch 2/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 426ms/step - accuracy: 0.6143 - loss: 1.0126 - val_accuracy: 0.6271 - val_loss: 0.9807 - learning_rate: 1.0000e-05\nEpoch 3/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 436ms/step - accuracy: 0.6207 - loss: 0.9867 - val_accuracy: 0.6387 - val_loss: 0.9582 - learning_rate: 1.0000e-05\nEpoch 4/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 439ms/step - accuracy: 0.6310 - loss: 0.9679 - val_accuracy: 0.6468 - val_loss: 0.9480 - learning_rate: 1.0000e-05\nEpoch 5/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 430ms/step - accuracy: 0.6519 - loss: 0.9294 - val_accuracy: 0.6558 - val_loss: 0.9231 - learning_rate: 1.0000e-05\nEpoch 6/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 437ms/step - accuracy: 0.6507 - loss: 0.9335 - val_accuracy: 0.6603 - val_loss: 0.9374 - learning_rate: 1.0000e-05\nEpoch 7/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 440ms/step - accuracy: 0.6591 - loss: 0.9073 - val_accuracy: 0.6650 - val_loss: 0.9220 - learning_rate: 1.0000e-05\nEpoch 8/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 433ms/step - accuracy: 0.6662 - loss: 0.8990 - val_accuracy: 0.6671 - val_loss: 0.9188 - learning_rate: 1.0000e-05\nEpoch 9/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 433ms/step - accuracy: 0.6562 - loss: 0.9123 - val_accuracy: 0.6713 - val_loss: 0.9137 - learning_rate: 1.0000e-05\nEpoch 10/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.6615 - loss: 0.8995 - val_accuracy: 0.6768 - val_loss: 0.8987 - learning_rate: 1.0000e-05\nEpoch 11/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 430ms/step - accuracy: 0.6741 - loss: 0.8727 - val_accuracy: 0.6766 - val_loss: 0.8846 - learning_rate: 1.0000e-05\nEpoch 12/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 439ms/step - accuracy: 0.6762 - loss: 0.8682 - val_accuracy: 0.6817 - val_loss: 0.8704 - learning_rate: 1.0000e-05\nEpoch 13/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.6765 - loss: 0.8571 - val_accuracy: 0.6829 - val_loss: 0.8834 - learning_rate: 1.0000e-05\nEpoch 14/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.6779 - loss: 0.8580 - val_accuracy: 0.6837 - val_loss: 0.8643 - learning_rate: 1.0000e-05\nEpoch 15/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.6842 - loss: 0.8463 - val_accuracy: 0.6890 - val_loss: 0.8618 - learning_rate: 1.0000e-05\nEpoch 16/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.6877 - loss: 0.8450 - val_accuracy: 0.6875 - val_loss: 0.9020 - learning_rate: 1.0000e-05\nEpoch 17/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 430ms/step - accuracy: 0.6948 - loss: 0.8253 - val_accuracy: 0.6888 - val_loss: 0.8596 - learning_rate: 1.0000e-05\nEpoch 18/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 433ms/step - accuracy: 0.6919 - loss: 0.8331 - val_accuracy: 0.6892 - val_loss: 0.9016 - learning_rate: 1.0000e-05\nEpoch 19/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 426ms/step - accuracy: 0.6902 - loss: 0.8303 - val_accuracy: 0.6921 - val_loss: 0.8606 - learning_rate: 1.0000e-05\nEpoch 20/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.7023 - loss: 0.8058\nEpoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 436ms/step - accuracy: 0.7023 - loss: 0.8058 - val_accuracy: 0.6943 - val_loss: 0.8660 - learning_rate: 1.0000e-05\nEpoch 21/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.6961 - loss: 0.8216 - val_accuracy: 0.6943 - val_loss: 0.8419 - learning_rate: 5.0000e-06\nEpoch 22/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 433ms/step - accuracy: 0.6957 - loss: 0.8083 - val_accuracy: 0.6956 - val_loss: 0.8754 - learning_rate: 5.0000e-06\nEpoch 23/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 428ms/step - accuracy: 0.7011 - loss: 0.7994 - val_accuracy: 0.6939 - val_loss: 0.8451 - learning_rate: 5.0000e-06\nEpoch 24/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 428ms/step - accuracy: 0.7065 - loss: 0.7892 - val_accuracy: 0.6960 - val_loss: 0.8296 - learning_rate: 5.0000e-06\nEpoch 25/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 430ms/step - accuracy: 0.7055 - loss: 0.7944 - val_accuracy: 0.6955 - val_loss: 0.8333 - learning_rate: 5.0000e-06\nEpoch 26/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.7048 - loss: 0.7914 - val_accuracy: 0.6984 - val_loss: 0.8260 - learning_rate: 5.0000e-06\nEpoch 27/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 427ms/step - accuracy: 0.7102 - loss: 0.7914 - val_accuracy: 0.6977 - val_loss: 0.8492 - learning_rate: 5.0000e-06\nEpoch 28/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 435ms/step - accuracy: 0.7094 - loss: 0.7987 - val_accuracy: 0.6956 - val_loss: 0.8397 - learning_rate: 5.0000e-06\nEpoch 29/80\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.7076 - loss: 0.7940\nEpoch 29: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 432ms/step - accuracy: 0.7076 - loss: 0.7940 - val_accuracy: 0.6968 - val_loss: 0.8313 - learning_rate: 5.0000e-06\nEpoch 29: early stopping\nRestoring model weights from the end of the best epoch: 26.\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 175ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 137ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 134ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 140ms/step\n\n--- STEP: Predicting Single Image ---\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step\nPredicted Class: clear\nConfidence: 49.27%\nclear: 49.27%\novercast: 19.61%\npartly cloudy: 26.25%\nrainy: 0.88%\nsnowy: 0.15%\nunknown: 3.83%\n\nClassification Report Summary:\nclear:\n  Precision: 0.6612\n  Recall: 0.7650\n  F1-Score: 0.7093\novercast:\n  Precision: 0.6056\n  Recall: 0.5650\n  F1-Score: 0.5846\npartly cloudy:\n  Precision: 0.6243\n  Recall: 0.7412\n  F1-Score: 0.6777\nrainy:\n  Precision: 0.8541\n  Recall: 0.6730\n  F1-Score: 0.7528\nsnowy:\n  Precision: 0.8108\n  Recall: 0.7330\n  F1-Score: 0.7700\nunknown:\n  Precision: 0.6819\n  Recall: 0.7180\n  F1-Score: 0.6995\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 142ms/step - accuracy: 0.7031 - loss: 0.7806\n\nValidation Accuracy: 0.6983506083488464\n","output_type":"stream"}],"execution_count":1}]}