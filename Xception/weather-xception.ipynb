{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7209801,"sourceType":"datasetVersion","datasetId":4171601}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Weather Classification Project**\n-------------------------------------","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau, \n    EarlyStopping, \n    ModelCheckpoint, \n    TensorBoard\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    roc_curve, \n    auc, \n    precision_recall_curve, \n    average_precision_score\n)\nimport shutil\nfrom datetime import datetime\n\n# Memory and GPU configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass WeatherClassificationPipeline:\n    def __init__(self, dataset_path, output_path, target_size=(299, 299), sample_size=30000):\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.target_size = target_size\n        self.sample_size = sample_size\n        \n        # Predefined classes to ensure consistency\n        self.classes = ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\n\n    def extract_sample_dataset(self):\n        \"\"\"Extract a stratified sample of images for train and validation\"\"\"\n        print(\"\\n--- STEP: Extracting Sample Dataset ---\")\n        \n        # Create output directories\n        sample_train_path = os.path.join(self.output_path, 'train')\n        sample_val_path = os.path.join(self.output_path, 'val')\n        os.makedirs(sample_train_path, exist_ok=True)\n        os.makedirs(sample_val_path, exist_ok=True)\n        \n        # Calculate total samples and samples per class for train and val\n        train_sample_ratio = 0.8\n        val_sample_ratio = 0.2\n        total_samples_per_class = self.sample_size // len(self.classes)\n        train_samples_per_class = int(total_samples_per_class * train_sample_ratio)\n        val_samples_per_class = int(total_samples_per_class * val_sample_ratio)\n\n        for cls in self.classes:\n            # Create class directories\n            train_cls_path = os.path.join(sample_train_path, cls)\n            val_cls_path = os.path.join(sample_val_path, cls)\n            os.makedirs(train_cls_path, exist_ok=True)\n            os.makedirs(val_cls_path, exist_ok=True)\n\n            # Find source directory\n            cls_source_path = os.path.join(self.dataset_path, 'train', cls)\n            \n            # Get all image files\n            image_files = [f for f in os.listdir(cls_source_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Randomly shuffle images\n            np.random.shuffle(image_files)\n            \n            # Select train and validation samples\n            train_images = image_files[:train_samples_per_class]\n            val_images = image_files[train_samples_per_class:train_samples_per_class+val_samples_per_class]\n            \n            # Copy train images\n            for img in train_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(train_cls_path, img)\n                shutil.copy(src, dst)\n            \n            # Copy validation images\n            for img in val_images:\n                src = os.path.join(cls_source_path, img)\n                dst = os.path.join(val_cls_path, img)\n                shutil.copy(src, dst)\n            \n            print(f\"Class {cls}:\")\n            print(f\"  Train images: {len(train_images)}\")\n            print(f\"  Validation images: {len(val_images)}\")\n\n        return self.output_path\n\n    def prepare_data_generators(self, dataset_path):\n        \"\"\"Prepare data generators for training and validation\"\"\"\n        print(\"\\n--- STEP: Preparing Data Generator ---\")\n        \n        # Create ImageDataGenerator with preprocessing and augmentation\n        train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True\n        )\n\n        val_datagen = ImageDataGenerator(\n            rescale=1./255\n        )\n\n        # Train generator\n        train_generator = train_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'train'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=True,\n            classes=self.classes\n        )\n\n        # Validation generator\n        val_generator = val_datagen.flow_from_directory(\n            os.path.join(dataset_path, 'val'),\n            target_size=self.target_size,\n            batch_size=32,\n            class_mode='categorical',\n            shuffle=False,\n            classes=self.classes\n        )\n        \n        print(f\"Classes: {self.classes}\")\n        print(f\"Number of training samples: {train_generator.samples}\")\n        print(f\"Number of validation samples: {val_generator.samples}\")\n\n        return train_generator, val_generator\n\n\n    # Xception Transfer Learning Model\n    def create_xception_transfer_model(self):\n        \"\"\"Create Xception transfer learning model\"\"\"\n        print(\"\\n--- STEP: Training Model ---\")\n        # Load pre-trained Xception model\n        base_model = Xception(\n            weights='imagenet', \n            include_top=False, \n            input_shape=(*self.target_size, 3)\n        )\n        \n        # Freeze base model layers\n        base_model.trainable = False\n        \n        # Add custom classification layers\n        x = base_model.output\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(1024, activation='relu')(x)\n        x = layers.Dropout(0.5)(x)\n        x = layers.Dense(512, activation='relu')(x)\n        x = layers.Dropout(0.3)(x)\n        outputs = layers.Dense(len(self.classes), activation='softmax')(x)\n        \n        model = models.Model(inputs=base_model.input, outputs=outputs)\n        return model\n\n    # Fine-Tuning Strategy\n    def fine_tune_model(self, model, train_generator, val_generator):\n        \"\"\"Apply fine-tuning strategy\"\"\"\n        print(\"\\n--- STEP: Fine-Tuning Model ---\")\n        # Unfreeze last few layers of base model for fine-tuning\n        for layer in model.layers[-50:]:\n            layer.trainable = True\n        \n        # Compile with lower learning rate\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=1e-5),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Fine-tuning callbacks\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.1, \n            patience=3, \n            min_lr=0.0000001\n        )\n        \n        early_stopping = EarlyStopping(\n            monitor='val_loss', \n            patience=5, \n            restore_best_weights=True\n        )\n        \n        # Fine-tuning training\n        fine_tune_history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=30,\n            callbacks=[reduce_lr, early_stopping]\n        )\n        \n        return fine_tune_history\n\n    def plot_training_curves(self, history):\n        \"\"\"Plot training and validation accuracy/loss\"\"\"\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(history['accuracy'], label='Training Accuracy')\n        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Model Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(history['loss'], label='Training Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.title('Model Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig('training_curves.png')\n        plt.close()\n\n    def plot_confusion_matrix(self, val_generator, model):\n        \"\"\"Plot confusion matrix\"\"\"\n        # Get predictions\n        predictions = model.predict(val_generator)\n        y_pred = np.argmax(predictions, axis=1)\n        y_true = val_generator.classes\n\n        # Plot confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', \n                    xticklabels=self.classes, \n                    yticklabels=self.classes)\n        plt.title('Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.tight_layout()\n        plt.savefig('confusion_matrix.png')\n        plt.close()\n\n    def plot_roc_curve(self, val_generator, model):\n        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute ROC curve and ROC area for each class\n        n_classes = len(self.classes)\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        \n        # Binarize the output\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n            plt.plot(fpr[i], tpr[i], \n                     label=f'ROC curve (class: {self.classes[i]}, area = {roc_auc[i]:.2f})')\n        \n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend(loc=\"lower right\")\n        plt.tight_layout()\n        plt.savefig('roc_curve.png')\n        plt.close()\n\n    def plot_precision_recall_curve(self, val_generator, model):\n        \"\"\"Plot Precision-Recall curves for multi-class classification\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_true = val_generator.classes\n        \n        plt.figure(figsize=(10, 8))\n        \n        # Compute Precision-Recall curve\n        n_classes = len(self.classes)\n        y_true_bin = to_categorical(y_true, num_classes=n_classes)\n        \n        for i in range(n_classes):\n            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n            avg_precision = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n            \n            plt.plot(recall, precision, \n                     label=f'Precision-Recall curve (class: {self.classes[i]}, AP = {avg_precision:.2f})')\n        \n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curve')\n        plt.legend(loc=\"best\")\n        plt.tight_layout()\n        plt.savefig('precision_recall_curve.png')\n        plt.close()\n\n    def generate_classification_report(self, val_generator, model):\n        \"\"\"Generate and save detailed classification report\"\"\"\n        y_pred_proba = model.predict(val_generator)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        y_true = val_generator.classes\n        \n        # Generate classification report\n        report = classification_report(\n            y_true, \n            y_pred, \n            target_names=self.classes, \n            output_dict=True\n        )\n        \n        # Create a DataFrame for better visualization\n        report_df = pd.DataFrame(report).transpose()\n        \n        # Plot as heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(report_df.iloc[:-2, :-1].astype(float), \n                    annot=True, \n                    cmap='YlGnBu', \n                    fmt='.2f')\n        plt.title('Classification Report Metrics')\n        plt.tight_layout()\n        plt.savefig('classification_report_heatmap.png')\n        plt.close()\n        \n        # Save textual report\n        with open('classification_report.txt', 'w') as f:\n            f.write(classification_report(\n                y_true, \n                y_pred, \n                target_names=self.classes\n            ))\n        \n        return report  \n\n    def visualize_sample_images(self, dataset_path):\n        \"\"\"Visualize sample images from each class\"\"\"\n        print(\"\\n--- STEP: Visualizing Sample Images ---\")\n        \n        # Create figure for sample images\n        plt.figure(figsize=(15, 10))\n        \n        # Get train directory\n        train_dir = os.path.join(dataset_path, 'train')\n        \n        # Iterate through classes\n        for i, cls in enumerate(self.classes, 1):\n            # Get path to class directory\n            cls_path = os.path.join(train_dir, cls)\n            \n            # Get list of image files\n            image_files = [f for f in os.listdir(cls_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            # Select first image\n            if image_files:\n                img_path = os.path.join(cls_path, image_files[0])\n                \n                # Load and display image\n                plt.subplot(2, 3, i)\n                img = plt.imread(img_path)\n                plt.imshow(img)\n                plt.title(cls)\n                plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('sample_images.png')\n        plt.close()\n\n    def predict_single_image(self, model, image_path):\n        \"\"\"Predict weather class for a single image\"\"\"\n        print(\"\\n--- STEP: Predicting Single Image ---\")\n        \n        # Load and preprocess the image\n        img = load_img(image_path, target_size=self.target_size)\n        img_array = img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize\n        \n        # Make prediction\n        predictions = model.predict(img_array)\n        predicted_class_index = np.argmax(predictions[0])\n        predicted_class = self.classes[predicted_class_index]\n        confidence = predictions[0][predicted_class_index]\n        \n        # Visualize prediction\n        plt.figure(figsize=(10, 5))\n        \n        # Original image\n        plt.subplot(1, 2, 1)\n        plt.imshow(plt.imread(image_path))\n        plt.title('Original Image')\n        plt.axis('off')\n        \n        # Prediction bar plot\n        plt.subplot(1, 2, 2)\n        plt.bar(self.classes, predictions[0])\n        plt.title('Class Probabilities')\n        plt.xlabel('Weather Classes')\n        plt.ylabel('Probability')\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig('single_image_prediction.png')\n        plt.close()\n        \n        # Print prediction details\n        print(f\"Predicted Class: {predicted_class}\")\n        print(f\"Confidence: {confidence:.2%}\")\n        \n        # Print full probabilities\n        for cls, prob in zip(self.classes, predictions[0]):\n            print(f\"{cls}: {prob:.2%}\")\n        \n        return predicted_class, confidence\n           \n\n    def run_pipeline(self):\n        # Extract sample dataset\n        sample_train_path = self.extract_sample_dataset()\n        \n        # Visualize sample images\n        self.visualize_sample_images(sample_train_path)\n\n        # Prepare data generators\n        train_generator, val_generator = self.prepare_data_generators(sample_train_path)\n\n        # Create model\n        model = self.create_xception_transfer_model()\n            \n        # Compile model\n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=0.001),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        # callbacks\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=3, \n            min_lr=0.000001\n        )\n        \n        early_stopping = EarlyStopping(\n            monitor='val_loss', \n            patience=5, \n            restore_best_weights=True\n        )\n      \n            \n        # Initial training\n        initial_history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=20,\n            callbacks=[reduce_lr, early_stopping]\n            \n        )\n\n        # Fine-tune the model\n        fine_tune_history = self.fine_tune_model(model, train_generator, val_generator)\n\n        # Combine training histories\n        combined_history = {\n            'accuracy': initial_history.history['accuracy'] + fine_tune_history.history['accuracy'],\n            'val_accuracy': initial_history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'],\n            'loss': initial_history.history['loss'] + fine_tune_history.history['loss'],\n            'val_loss': initial_history.history['val_loss'] + fine_tune_history.history['val_loss']\n        }\n        \n        \n        # Visualizations with combined history\n        self.plot_training_curves(combined_history)\n        self.plot_confusion_matrix(val_generator, model)\n        self.plot_roc_curve(val_generator, model)\n        self.plot_precision_recall_curve(val_generator, model)\n        report = self.generate_classification_report(val_generator, model)\n            \n        # Predict single image\n        test_image_path = '/kaggle/input/bdd100k-weather-classification/test/cabc30fc-e7726578.jpg'\n        self.predict_single_image(model, test_image_path)\n            \n        # Print out some key metrics from the report\n        print(\"\\nClassification Report Summary:\")\n        for cls in self.classes:\n            print(f\"{cls}:\")\n            print(f\"  Precision: {report[cls]['precision']:.4f}\")\n            print(f\"  Recall: {report[cls]['recall']:.4f}\")\n            print(f\"  F1-Score: {report[cls]['f1-score']:.4f}\")\n            \n        # Evaluate model\n        val_loss, val_accuracy = model.evaluate(val_generator)\n        print(f\"\\nValidation Accuracy: {val_accuracy}\")\n            \n        # Save model\n        model.save('weather_classification_model.h5')\n            \n        return model\n        \n# Main execution\nif __name__ == \"__main__\":\n    # Create output directory for sample dataset\n    output_path = '/kaggle/working/bdd100k_sample'\n    os.makedirs(output_path, exist_ok=True)\n\n    # Initialize and run pipeline\n    pipeline = WeatherClassificationPipeline(\n        dataset_path='/kaggle/input/bdd100k-weather-classification',\n        output_path=output_path\n    )\n    pipeline.run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:13:14.682938Z","iopub.execute_input":"2024-12-18T08:13:14.683275Z","iopub.status.idle":"2024-12-18T15:20:22.900429Z","shell.execute_reply.started":"2024-12-18T08:13:14.683242Z","shell.execute_reply":"2024-12-18T15:20:22.899680Z"}},"outputs":[{"name":"stdout","text":"\n--- STEP: Extracting Sample Dataset ---\nClass clear:\n  Train images: 4000\n  Validation images: 1000\nClass overcast:\n  Train images: 4000\n  Validation images: 1000\nClass partly cloudy:\n  Train images: 4000\n  Validation images: 881\nClass rainy:\n  Train images: 4000\n  Validation images: 1000\nClass snowy:\n  Train images: 4000\n  Validation images: 1000\nClass unknown:\n  Train images: 4000\n  Validation images: 1000\n\n--- STEP: Visualizing Sample Images ---\n\n--- STEP: Preparing Data Generator ---\nFound 24000 images belonging to 6 classes.\nFound 5881 images belonging to 6 classes.\nClasses: ['clear', 'overcast', 'partly cloudy', 'rainy', 'snowy', 'unknown']\nNumber of training samples: 24000\nNumber of validation samples: 5881\n\n--- STEP: Training Model ---\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1734509912.330287     123 service.cc:145] XLA service 0x7e4870001350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1734509912.330356     123 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1734509912.330362     123 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n2024-12-18 08:18:43.121949: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng3{k11=2} for conv (f32[32,128,147,147]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,147,147]{3,2,1,0}, f32[128,128,1,1]{3,2,1,0}), window={size=1x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-12-18 08:18:43.299730: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.177977353s\nTrying algorithm eng3{k11=2} for conv (f32[32,128,147,147]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,147,147]{3,2,1,0}, f32[128,128,1,1]{3,2,1,0}), window={size=1x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\nI0000 00:00:1734509933.907483     123 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 743ms/step - accuracy: 0.3883 - loss: 1.5020 - val_accuracy: 0.4722 - val_loss: 1.3019 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 711ms/step - accuracy: 0.4760 - loss: 1.3139 - val_accuracy: 0.5018 - val_loss: 1.2570 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 713ms/step - accuracy: 0.4939 - loss: 1.2822 - val_accuracy: 0.5217 - val_loss: 1.2468 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 718ms/step - accuracy: 0.5097 - loss: 1.2518 - val_accuracy: 0.5217 - val_loss: 1.2096 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 716ms/step - accuracy: 0.5172 - loss: 1.2496 - val_accuracy: 0.5256 - val_loss: 1.2127 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 714ms/step - accuracy: 0.5256 - loss: 1.2340 - val_accuracy: 0.5315 - val_loss: 1.1997 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 716ms/step - accuracy: 0.5291 - loss: 1.2140 - val_accuracy: 0.5450 - val_loss: 1.1828 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 719ms/step - accuracy: 0.5296 - loss: 1.2053 - val_accuracy: 0.5263 - val_loss: 1.2044 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 716ms/step - accuracy: 0.5331 - loss: 1.1941 - val_accuracy: 0.5530 - val_loss: 1.1744 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 720ms/step - accuracy: 0.5359 - loss: 1.1891 - val_accuracy: 0.5487 - val_loss: 1.1785 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 715ms/step - accuracy: 0.5476 - loss: 1.1751 - val_accuracy: 0.5428 - val_loss: 1.1733 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 716ms/step - accuracy: 0.5439 - loss: 1.1731 - val_accuracy: 0.5521 - val_loss: 1.1678 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 716ms/step - accuracy: 0.5417 - loss: 1.1759 - val_accuracy: 0.5443 - val_loss: 1.1783 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 707ms/step - accuracy: 0.5441 - loss: 1.1662 - val_accuracy: 0.5608 - val_loss: 1.1428 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 717ms/step - accuracy: 0.5510 - loss: 1.1581 - val_accuracy: 0.5570 - val_loss: 1.1591 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 723ms/step - accuracy: 0.5536 - loss: 1.1550 - val_accuracy: 0.5591 - val_loss: 1.1463 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 715ms/step - accuracy: 0.5479 - loss: 1.1574 - val_accuracy: 0.5378 - val_loss: 1.1609 - learning_rate: 0.0010\nEpoch 18/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 723ms/step - accuracy: 0.5650 - loss: 1.1213 - val_accuracy: 0.5628 - val_loss: 1.1269 - learning_rate: 2.0000e-04\nEpoch 19/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 714ms/step - accuracy: 0.5716 - loss: 1.0805 - val_accuracy: 0.5664 - val_loss: 1.1166 - learning_rate: 2.0000e-04\nEpoch 20/20\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 711ms/step - accuracy: 0.5786 - loss: 1.0873 - val_accuracy: 0.5673 - val_loss: 1.1150 - learning_rate: 2.0000e-04\n\n--- STEP: Fine-Tuning Model ---\nEpoch 1/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 726ms/step - accuracy: 0.5122 - loss: 1.2714 - val_accuracy: 0.5752 - val_loss: 1.0923 - learning_rate: 1.0000e-05\nEpoch 2/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 716ms/step - accuracy: 0.5938 - loss: 1.0649 - val_accuracy: 0.6079 - val_loss: 1.0068 - learning_rate: 1.0000e-05\nEpoch 3/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 721ms/step - accuracy: 0.6249 - loss: 0.9904 - val_accuracy: 0.6467 - val_loss: 0.9323 - learning_rate: 1.0000e-05\nEpoch 4/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 722ms/step - accuracy: 0.6521 - loss: 0.9316 - val_accuracy: 0.6655 - val_loss: 0.8935 - learning_rate: 1.0000e-05\nEpoch 5/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 720ms/step - accuracy: 0.6753 - loss: 0.8800 - val_accuracy: 0.6701 - val_loss: 0.8773 - learning_rate: 1.0000e-05\nEpoch 6/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 714ms/step - accuracy: 0.6819 - loss: 0.8574 - val_accuracy: 0.6827 - val_loss: 0.8395 - learning_rate: 1.0000e-05\nEpoch 7/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 713ms/step - accuracy: 0.6962 - loss: 0.8274 - val_accuracy: 0.6943 - val_loss: 0.8243 - learning_rate: 1.0000e-05\nEpoch 8/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 714ms/step - accuracy: 0.6981 - loss: 0.8102 - val_accuracy: 0.6970 - val_loss: 0.8229 - learning_rate: 1.0000e-05\nEpoch 9/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 714ms/step - accuracy: 0.7066 - loss: 0.7847 - val_accuracy: 0.7096 - val_loss: 0.7967 - learning_rate: 1.0000e-05\nEpoch 10/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 723ms/step - accuracy: 0.7182 - loss: 0.7600 - val_accuracy: 0.7148 - val_loss: 0.7906 - learning_rate: 1.0000e-05\nEpoch 11/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 724ms/step - accuracy: 0.7227 - loss: 0.7548 - val_accuracy: 0.7160 - val_loss: 0.7850 - learning_rate: 1.0000e-05\nEpoch 12/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 729ms/step - accuracy: 0.7267 - loss: 0.7393 - val_accuracy: 0.7172 - val_loss: 0.7838 - learning_rate: 1.0000e-05\nEpoch 13/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 726ms/step - accuracy: 0.7396 - loss: 0.7118 - val_accuracy: 0.7223 - val_loss: 0.7731 - learning_rate: 1.0000e-05\nEpoch 14/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 714ms/step - accuracy: 0.7426 - loss: 0.7023 - val_accuracy: 0.7247 - val_loss: 0.7717 - learning_rate: 1.0000e-05\nEpoch 15/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 734ms/step - accuracy: 0.7425 - loss: 0.6987 - val_accuracy: 0.7216 - val_loss: 0.7789 - learning_rate: 1.0000e-05\nEpoch 16/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 723ms/step - accuracy: 0.7435 - loss: 0.6803 - val_accuracy: 0.7227 - val_loss: 0.7675 - learning_rate: 1.0000e-05\nEpoch 17/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 722ms/step - accuracy: 0.7526 - loss: 0.6663 - val_accuracy: 0.7264 - val_loss: 0.7688 - learning_rate: 1.0000e-05\nEpoch 18/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 729ms/step - accuracy: 0.7585 - loss: 0.6524 - val_accuracy: 0.7210 - val_loss: 0.7708 - learning_rate: 1.0000e-05\nEpoch 19/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 724ms/step - accuracy: 0.7596 - loss: 0.6403 - val_accuracy: 0.7237 - val_loss: 0.7674 - learning_rate: 1.0000e-05\nEpoch 20/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 719ms/step - accuracy: 0.7673 - loss: 0.6295 - val_accuracy: 0.7233 - val_loss: 0.7676 - learning_rate: 1.0000e-06\nEpoch 21/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 724ms/step - accuracy: 0.7706 - loss: 0.6175 - val_accuracy: 0.7220 - val_loss: 0.7671 - learning_rate: 1.0000e-06\nEpoch 22/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 719ms/step - accuracy: 0.7693 - loss: 0.6248 - val_accuracy: 0.7230 - val_loss: 0.7680 - learning_rate: 1.0000e-06\nEpoch 23/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 723ms/step - accuracy: 0.7714 - loss: 0.6150 - val_accuracy: 0.7232 - val_loss: 0.7681 - learning_rate: 1.0000e-06\nEpoch 24/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 724ms/step - accuracy: 0.7749 - loss: 0.6102 - val_accuracy: 0.7235 - val_loss: 0.7696 - learning_rate: 1.0000e-06\nEpoch 25/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 725ms/step - accuracy: 0.7740 - loss: 0.6103 - val_accuracy: 0.7240 - val_loss: 0.7708 - learning_rate: 1.0000e-07\nEpoch 26/30\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 724ms/step - accuracy: 0.7710 - loss: 0.6200 - val_accuracy: 0.7228 - val_loss: 0.7692 - learning_rate: 1.0000e-07\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 227ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 220ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 222ms/step\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 222ms/step\n\n--- STEP: Predicting Single Image ---\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\nPredicted Class: clear\nConfidence: 29.23%\nclear: 29.23%\novercast: 23.97%\npartly cloudy: 26.50%\nrainy: 3.96%\nsnowy: 2.21%\nunknown: 14.14%\n\nClassification Report Summary:\nclear:\n  Precision: 0.6869\n  Recall: 0.7700\n  F1-Score: 0.7261\novercast:\n  Precision: 0.6509\n  Recall: 0.5630\n  F1-Score: 0.6038\npartly cloudy:\n  Precision: 0.6960\n  Recall: 0.7980\n  F1-Score: 0.7435\nrainy:\n  Precision: 0.8483\n  Recall: 0.6710\n  F1-Score: 0.7493\nsnowy:\n  Precision: 0.8146\n  Recall: 0.7820\n  F1-Score: 0.7980\nunknown:\n  Precision: 0.6675\n  Recall: 0.7570\n  F1-Score: 0.7095\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 230ms/step - accuracy: 0.7156 - loss: 0.7780\n\nValidation Accuracy: 0.7219860553741455\n","output_type":"stream"}],"execution_count":1}]}